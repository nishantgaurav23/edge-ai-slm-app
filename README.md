# Edge AI SLM App ğŸš€

[![Python](https://img.shields.io/badge/Python-3.9+-blue.svg)](https://www.python.org/)
[![Kivy](https://img.shields.io/badge/Kivy-2.3.1-green.svg)](https://kivy.org/)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Status](https://img.shields.io/badge/Status-Production-success.svg)]()

> An offline-first mobile AI chat application using Small Language Models (SLM) with complete privacy, zero API costs, and intelligent resource management.

![App Demo](screenshots/demo.png)
*Chat interface with local AI running on-device*

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Key Features](#key-features)
- [Screenshots](#screenshots)
- [Architecture](#architecture)
- [Installation](#installation)
  - [Desktop (macOS/Linux/Windows)](#desktop-installation)
  - [Android](#android-installation)
- [Quick Start](#quick-start)
- [Configuration](#configuration)
- [Technical Details](#technical-details)
- [Troubleshooting](#troubleshooting)
- [Contributing](#contributing)
- [License](#license)

---

## ğŸ¯ Overview

Edge AI SLM App demonstrates how to build a production-ready, privacy-first AI assistant that runs entirely on-device. No internet required, no API costs, complete data ownership.

### The Challenge

Build an offline-first mobile application using Small Language Models that:
- âœ… Works without internet connectivity
- âœ… Costs $0 in API fees
- âœ… Maintains complete user privacy
- âœ… Optimizes for resource-constrained hardware
- âœ… Manages memory pressure intelligently
- âœ… Adapts to device capabilities

### Why This Matters

This project proves understanding of:
- **Edge AI principles** - Running AI where the data is
- **Resource optimization** - Working within hardware constraints
- **Privacy-first design** - No data leaves the device
- **Mobile engineering** - Building for limited resources

---

## âœ¨ Key Features

### ğŸ¤– AI Capabilities
- **Offline Inference** - Run TinyLlama 1.1B completely offline
- **Few-Shot Learning** - Guided responses with examples
- **Context Management** - Semantic chunking with embeddings
- **Multi-Turn Conversations** - Maintains conversation history

### ğŸ”’ Privacy & Security
- **AES-256 Encryption** - All conversations encrypted locally
- **Zero Telemetry** - No data collection or tracking
- **Offline-First** - Works without any internet connection
- **Local Storage** - SQLite database with encryption

### âš¡ Performance Optimization
- **Lazy Loading** - Load models only when needed
- **Memory Management** - Auto-unload on memory pressure
- **GPU Acceleration** - Metal GPU support (Apple Silicon)
- **Battery Optimization** - Throttle during low battery

### ğŸ¨ User Experience
- **Modern UI** - Dark theme with Material Design (KivyMD)
- **Real-time Chat** - Non-blocking inference with threading
- **System Monitoring** - Display RAM, battery, power mode
- **Responsive Design** - Adapts to different screen sizes

---

## ğŸ“¸ Screenshots

### Main Interface
![Home Screen](screenshots/home_screen.png)
*Home screen with app title and start chat button*

### Chat Interface
![Chat Screen](screenshots/chat_screen.png)
*Chat interface showing conversation with AI*

### System Information
![System Info](screenshots/system_info.png)
*Device information and battery status*

### AI Response
![AI Response](screenshots/ai_response.png)
*Example AI responses to user queries*

---

## ğŸ—ï¸ Architecture

The app follows a clean architecture pattern with clear separation of concerns:

```
edge-ai-slm-app/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ core/              # Core business logic
â”‚   â”‚   â”œâ”€â”€ inference_engine.py      # Model loading & inference
â”‚   â”‚   â”œâ”€â”€ context_manager.py       # Context window management
â”‚   â”‚   â””â”€â”€ data_store.py            # Encrypted storage
â”‚   â”œâ”€â”€ services/          # Supporting services
â”‚   â”‚   â”œâ”€â”€ model_loader.py          # Model download & verification
â”‚   â”‚   â”œâ”€â”€ hardware_monitor.py      # Battery & device detection
â”‚   â”‚   â”œâ”€â”€ memory_monitor.py        # Memory pressure monitoring
â”‚   â”‚   â”œâ”€â”€ quantization_service.py  # Dynamic quantization
â”‚   â”‚   â””â”€â”€ sync_service.py          # Offline-first sync
â”‚   â””â”€â”€ ui/                # User interface
â”‚       â””â”€â”€ screens/
â”‚           â””â”€â”€ chat_screen.py       # Chat interface
â”œâ”€â”€ models/                # GGUF model files
â”œâ”€â”€ tests/                 # Unit tests
â””â”€â”€ main.py               # Application entry point
```

For detailed architecture documentation, see [ARCHITECTURE.md](ARCHITECTURE.md).

---

## ğŸ’» Installation

### Prerequisites

- Python 3.9 or higher
- 4GB RAM minimum (8GB+ recommended)
- 2GB free disk space (for models)

### Desktop Installation

#### 1. Clone the Repository

```bash
git clone https://github.com/nishantgaurav23/edge-ai-slm-app.git
cd edge-ai-slm-app
```

#### 2. Create Virtual Environment

**macOS/Linux:**
```bash
python3 -m venv venv
source venv/bin/activate
```

**Windows:**
```cmd
python -m venv venv
venv\Scripts\activate
```

#### 3. Install Dependencies

```bash
pip install --upgrade pip
pip install -r requirements.txt
```

#### 4. Download AI Model

Download TinyLlama 1.1B (Q4_K_M quantization):

```bash
cd models
curl -L -o tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \
  "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
cd ..
```

**Alternative models:**
- **Phi-2** (1.6GB): Better quality, requires more RAM
- **Llama-2-7B** (3.8GB): High quality, requires 8GB+ RAM

#### 5. Run the App

```bash
python main.py
```

---

### Android Installation

The app can be deployed to Android using Buildozer. Here's how:

#### 1. Install Buildozer

**On Linux/macOS:**
```bash
pip install buildozer
```

**On macOS (additional requirements):**
```bash
brew install autoconf automake libtool pkg-config
brew link libtool
```

#### 2. Install Android SDK

Buildozer will automatically download Android SDK/NDK on first build.

#### 3. Create Buildozer Spec

A `buildozer.spec` file is included in the repository. Key settings:

```ini
[app]
title = Edge AI SLM
package.name = edgeaislm
package.domain = com.nishantgaurav

# Permissions
android.permissions = INTERNET, WRITE_EXTERNAL_STORAGE, READ_EXTERNAL_STORAGE

# Requirements
requirements = python3,kivy,kivymd,llama-cpp-python,plyer,requests,psutil,sentence-transformers,cryptography

# Architecture
android.archs = arm64-v8a, armeabi-v7a
```

#### 4. Build APK

**Debug build:**
```bash
buildozer android debug
```

**Release build:**
```bash
buildozer android release
```

The APK will be in `bin/` directory.

#### 5. Install on Android Device

**Via USB (ADB):**
```bash
buildozer android deploy run
```

**Manual installation:**
1. Copy `bin/*.apk` to your Android device
2. Enable "Install from Unknown Sources" in Settings
3. Tap the APK file to install

#### 6. Download Model on Android

After installing the app:
1. Connect to WiFi
2. Open the app
3. Model will be downloaded automatically on first run
4. Alternatively, manually place model in:
   ```
   /sdcard/Android/data/com.nishantgaurav.edgeaislm/files/models/
   ```

#### Android Requirements

- **Minimum:** Android 5.0 (API 21)
- **Recommended:** Android 8.0+ (API 26+)
- **RAM:** 4GB minimum, 6GB+ recommended
- **Storage:** 2GB free space

#### Performance Notes

**On Android:**
- First load takes 15-30 seconds
- Inference: 3-8 seconds per response
- Battery drain: ~10-15% per hour of active use
- Works completely offline after model download

---

## ğŸš€ Quick Start

### First Time Setup

1. **Start the app:**
   ```bash
   python main.py
   ```

2. **Click "Start Chat"**

3. **Wait for model to load** (10-20 seconds first time)

4. **Start chatting!** Try:
   - "Hello"
   - "What is Python?"
   - "Tell me a joke"
   - "Explain machine learning"

### Example Conversations

```
You: Hello
AI: Hello! How can I help you today?

You: What is 2+2?
AI: 2+2 equals 4.

You: Tell me about Python
AI: Python is a high-level, interpreted programming language known for its simplicity and readability...
```

---

## âš™ï¸ Configuration

### Model Selection

Edit `app/ui/screens/chat_screen.py`:

```python
self.model_filename = "tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
```

### Generation Parameters

Adjust in `chat_screen.py`:

```python
response = self.inference_engine.generate_response(
    prompt=prompt,
    max_tokens=100,        # Response length
    temperature=0.3,       # Creativity (0.0-2.0)
    top_p=0.9,            # Nucleus sampling
    repeat_penalty=1.3,    # Reduce repetition
)
```

### Context Window Size

Edit `chat_screen.py`:

```python
self.context_manager = ContextManager(
    max_tokens=2048,  # Adjust based on RAM
    ...
)
```

---

## ğŸ”§ Technical Details

### Models Supported

| Model | Size | RAM Required | Quality | Speed |
|-------|------|-------------|---------|-------|
| TinyLlama 1.1B (Q4) | 638MB | 2GB | Good | Fast |
| TinyLlama 1.1B (Q8) | 1.1GB | 3GB | Better | Fast |
| Phi-2 (Q4) | 1.6GB | 4GB | Excellent | Medium |
| Llama-2-7B (Q4) | 3.8GB | 8GB | Best | Slow |

### Hardware Acceleration

- **macOS (Apple Silicon):** Metal GPU acceleration
- **Linux/Windows:** CPU-based inference
- **Android:** NNAPI (if available)

### Memory Management

The app intelligently manages memory:

1. **Lazy Loading:** Model loaded only when needed
2. **Auto-Unload:** Unloads model when RAM > 85%
3. **Preloading:** Loads model during idle when RAM < 50%
4. **Context Pruning:** Semantic-aware context trimming

### Battery Optimization

- **Full Mode:** Normal inference (charging or >50% battery)
- **Balanced Mode:** Throttled (20-50% battery)
- **Power Save:** Queued requests (<20% battery)

---

## ğŸ› Troubleshooting

### Model Not Loading

**Problem:** "Model not found" error

**Solution:**
```bash
cd models
ls -lh  # Check if model file exists
# If missing, download again
curl -L -o tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf \
  "https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF/resolve/main/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"
```

### Slow Performance

**Problem:** Responses take > 10 seconds

**Solutions:**
- Reduce `max_tokens` to 50-100
- Use smaller model (TinyLlama Q4 instead of Q8)
- Close other applications
- Increase `temperature` to 0.5+

### Random Responses

**Problem:** AI gives unrelated answers

**Solutions:**
- Lower `temperature` to 0.2-0.4
- Increase `repeat_penalty` to 1.5
- Clear conversation: `rm edge_ai_data.db`
- Use more specific prompts

### Memory Errors

**Problem:** App crashes or freezes

**Solutions:**
```python
# Reduce context size in chat_screen.py
max_tokens=1024  # Instead of 2048

# Use smaller model
# Download TinyLlama Q4 instead of Q8

# Check available RAM
python -c "import psutil; print(f'{psutil.virtual_memory().available / (1024**3):.1f}GB free')"
```

### UI Not Showing

**Problem:** Black screen after "Start Chat"

**Solution:**
```bash
# Restart app
# Check logs
tail -50 ~/.kivy/logs/kivy_*.txt

# Reinstall dependencies
pip uninstall kivy kivymd
pip install kivy kivymd
```

---

## ğŸ§ª Testing

Run the test suite:

```bash
# All tests
pytest tests/ -v

# Specific test
pytest tests/test_context_manager.py -v

# With coverage
pytest tests/ --cov=app --cov-report=html
```

---

## ğŸ“Š Performance Benchmarks

### Desktop (Apple M3, 16GB RAM)

- **Model Loading:** 8-12 seconds
- **Inference Speed:** 15-25 tokens/second
- **Memory Usage:** 800MB-1.2GB
- **CPU Usage:** 60-80% (1 core)

### Android (Snapdragon 888, 8GB RAM)

- **Model Loading:** 20-30 seconds
- **Inference Speed:** 5-10 tokens/second
- **Memory Usage:** 1.2GB-1.8GB
- **Battery Drain:** 12% per hour

---

## ğŸ¤ Contributing

This is a personal learning project by [nishantgaurav23](https://github.com/nishantgaurav23).

### Reporting Issues

Found a bug? Please open an issue with:
- Steps to reproduce
- Expected vs actual behavior
- System info (OS, RAM, Python version)
- Logs (from `.kivy/logs/`)

---

## ğŸ“š Further Reading

- [ARCHITECTURE.md](ARCHITECTURE.md) - Detailed system architecture
- [CHALLENGE.md](CHALLENGE.md) - Design decisions and trade-offs
- [Kivy Documentation](https://kivy.org/doc/stable/)
- [llama.cpp Documentation](https://github.com/ggerganov/llama.cpp)
- [Edge AI Principles](https://www.edgeimpulse.com/)

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **TinyLlama** - Lightweight LLM model
- **llama.cpp** - Efficient inference engine
- **Kivy/KivyMD** - Cross-platform UI framework
- **sentence-transformers** - Semantic embeddings
- **TheBloke** - GGUF model quantizations

---

## ğŸ“ Contact

**Nishant Gaurav**
- GitHub: [@nishantgaurav23](https://github.com/nishantgaurav23)
- Project: [edge-ai-slm-app](https://github.com/nishantgaurav23/edge-ai-slm-app)

---

## ğŸ¯ Project Status

- âœ… Core functionality complete
- âœ… Desktop support (macOS/Linux/Windows)
- âœ… Android build configuration ready
- ğŸš§ iOS support (planned)
- ğŸš§ Model gallery UI (planned)
- ğŸš§ Voice input (planned)

---

**Built with â¤ï¸ for privacy, performance, and edge AI**
# edge-ai-slm-app
